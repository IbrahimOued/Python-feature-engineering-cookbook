{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputing Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data refers to the absence of values for certain observations and is an unavoidable problem in most data sources. Scikit-learn does not support missing values as input, so we need to remove observations with missing data or transform them into permitted values. The act of replacing missing data with statistical estimates of missing values is called **imputation**. The goal of any imputation technique is to produce a complete dataset that can be used to train machine learning models. **There are multiple imputation techniques we can apply to our data**. **The choice of imputation technique** we use will depend on whether **the data is missing at random**, **the number of missing values**, and the **machine learning model we intend to use**. In this chapter, we will discuss several missing data imputation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('../data/crx.data', header=None)\n",
    "\n",
    "varnames = ['A'+str(s) for s in range(1, 17)]\n",
    "\n",
    "data.columns = varnames\n",
    "\n",
    "data = data.replace('?', np.nan)\n",
    "\n",
    "data['A2'] = data['A2'].astype('float')\n",
    "data['A14'] = data['A14'].astype('float')\n",
    "\n",
    "data['A16'] = data['A16'].map({'+':1, '-':0})\n",
    "\n",
    "random.seed(9001)\n",
    "values = [random.randint(0, len(data)) for p in range(0, 100)]\n",
    "for var in ['A3', 'A8', 'A9', 'A10']:\n",
    "    data.loc[values, var] = np.nan\n",
    "\n",
    "data.to_csv('../data/creditApprovalUCI.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Removing observations with missing data**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete Case Analysis(CCA)**, also called **list-wise deletion of cases**, consists of **discarding those observations where the values in any of the variables are missing**. CCA can be applied to categorical and numerical variables. CCA is quick and easy to implement and has the advantage that it preserves the distribution of the variables, provided the data is missing at random and only a small proportion of the data is missing. However, if data is missing across many variables, CCA may lead to the removal of a big portion of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A11    0.000000\n",
       "A12    0.000000\n",
       "A13    0.000000\n",
       "A15    0.000000\n",
       "A16    0.000000\n",
       "A4     0.008696\n",
       "A5     0.008696\n",
       "A6     0.013043\n",
       "A7     0.013043\n",
       "A1     0.017391\n",
       "A2     0.017391\n",
       "A14    0.018841\n",
       "A3     0.133333\n",
       "A8     0.133333\n",
       "A9     0.133333\n",
       "A10    0.133333\n",
       "dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 First, we'll import the libraries\n",
    "import pandas as pd\n",
    "\n",
    "# 2 Load the Credit Approval Dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Calculate the percentage of missing values\n",
    "data.isnull().mean().sort_values(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 Now, we'll remove the observations with missing data in any of the variables:\n",
    "data_cca = data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To remove observations where data is missing in a subset of variables, we can execute `data.dropna(subset=['A3', 'A4'])`. To remove observations if data is missing in all the variables, we can execute `data.dropna(how='all')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total observations: 690\n",
      "Number of observations with complete cases: 564\n"
     ]
    }
   ],
   "source": [
    "# 5 Let's print and compare the size of the original and complete case datasets:\n",
    "print('Number of total observations: {}'.format(len(data)))\n",
    "print('Number of observations with complete cases: {}'.format(len(data_cca)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performing mean or median imputation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Mean` or `median` imputation consists of replacing missing values with the variable mean or median** . This can **only be performed in numerical variables**. The **mean or the median is calculated using a train set**, and these values are used to **impute missing data in train and test sets**, as well as in future data we intend to score with the machine learning model. Therefore, we need to store these `mean` and `median` values. `Scikit-learn` and `Feature-engine` transformers learn the parameters from the train set and store these parameters for future use. So, in this recipe, we will learn how to perform mean or median imputation using the scikit-learn and Feature-engine libraries and pandas for comparison.\n",
    "\n",
    "> <font color=cyan>Use mean imputation if variables are normally distributed and median imputation otherwise</font>. **Mean and median imputation may distort the distribution of the original variables if there is a high percentage of missing data**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Import the libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.imputation import MeanMedianImputer\n",
    "# 2 Let's load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 In mean and median imputation, the mean or median values should be\n",
    "# calculated using the variables in the train set, therefore let's separate\n",
    "# the data into and sets and their respective targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ou can check the size of the returned datasets using pandas' shape: `X_train.shape`, `X_test.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1     0.008282\n",
       "A2     0.022774\n",
       "A3     0.140787\n",
       "A4     0.008282\n",
       "A5     0.008282\n",
       "A6     0.008282\n",
       "A7     0.008282\n",
       "A8     0.140787\n",
       "A9     0.140787\n",
       "A10    0.140787\n",
       "A11    0.000000\n",
       "A12    0.000000\n",
       "A13    0.000000\n",
       "A14    0.014493\n",
       "A15    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4 Let's check the percentage of missing values in the train set\n",
    "X_train.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Let's replace the missing values with the median in five numerical variables using pandas:\n",
    "for var in ['A2', 'A3', 'A8', 'A11', 'A15']:\n",
    "    value = X_train[var].median()\n",
    "    X_train[var] = X_train[var].fillna(value)\n",
    "    X_test[var] = X_test[var].fillna(value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note **how we calculate the median using the train set and then use this value to replace the missing data in the train and test sets**.\n",
    "\n",
    "> To impute missing data with the mean, we use pandas' mean():`value = X_train[var].mean()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A1     0.0\n",
       "A2     0.0\n",
       "A3     0.0\n",
       "A4     0.0\n",
       "A5     0.0\n",
       "A6     0.0\n",
       "A7     0.0\n",
       "A8     0.0\n",
       "A9     0.0\n",
       "A10    0.0\n",
       "A11    0.0\n",
       "A12    0.0\n",
       "A13    0.0\n",
       "A14    0.0\n",
       "A15    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The pandas' `fillna()` returns a new dataset with imputed values by default. We can set the inplace argument to `True` to replace missing data in the original dataframe: `X_train[var].fillna(inplace=True)`.\n",
    "\n",
    "Now, let's impute missing values by the median using scikit-learn so that we can store learned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 To do this, let's separate the original dataset into train and test sets, keeping only the numerical variables:\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['A2', 'A3', 'A8', 'A11', 'A15']], data['A16'], \n",
    "    test_size=0.3, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `SimpleImputer()` from scikit-learn **will impute all variables in the dataset**. Therefore, **if we use mean or median imputation and the dataset contains categorical variables, we will get an error**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Let's create a median imputation transformer using SimpleImputer() from scikit-learn:\n",
    "imputer = SimpleImputer(strategy='median')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To perform mean imputation, we should set the strategy to mean: `imputer = SimpleImputer(strategy = 'mean')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28.835,  2.75 ,  1.   ,  0.   ,  6.   ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8 Let's fit the SimpleImputer() to the train set so that it learns the median values of the variables:\n",
    "imputer.fit(X_train)\n",
    "# 9 Let's inspect the learned median values:\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Let's replace missing values with medians:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `SimpleImputer()` returns NumPy arrays. We can transform the array into a dataframe using `pd.DataFrame(X_train, columns = ['A2', 'A3', 'A8', 'A11', 'A15'])`. Be mindful of the order of the variables.\n",
    "\n",
    "Finally, let's perform median imputation using `MeanMedianImputer()` from `Feature-engine`. First, we need to load and divide the dataset, just like we did in step 2 and step 3. Next, we need to create an imputation transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 Let's set up a median imputation transformer using MeanMedianImputer() from Feature-engine specifying the variables to impute:\n",
    "median_imputer = MeanMedianImputer(imputation_method='median', variables=['A2', 'A3', 'A8', 'A11', 'A15'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To perform mean imputation, change the imputation method, as follows: `MeanMedianImputer(imputation_method='mean')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A2': 28.835, 'A3': 2.75, 'A8': 1.0, 'A11': 0.0, 'A15': 6.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 Let's fit the median imputer so that it learns the median values for each of the specified variables:\n",
    "median_imputer.fit(X_train)\n",
    "# 13 Let's inspect the learned medians:\n",
    "median_imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 Finally, let's replace the missing values with the median:\n",
    "X_train = median_imputer.transform(X_train)\n",
    "X_test = median_imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A2     0.0\n",
       "A3     0.0\n",
       "A8     0.0\n",
       "A11    0.0\n",
       "A15    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[['A2','A3', 'A8', 'A11', 'A15']].isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **There's more**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn's `SimpleImputer()` **imputes all the variables in the dataset** but, with scikit-learn's `ColumnTransformer()`, we can `select specific variables we want to impute`. For details on how to use `ColumnTransformer()` with `SimpleImputer()`, see ***the Assembling an imputation pipeline with scikit-learn*** recipe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementing mode or frequent category imputation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mode imputation consists of replacing missing values with the mode. We normally use this procedure in categorical variables, hen the **frequent category imputation** name. Frequent categories are estimated using the train set and then used to impute values in train, test and future datasets. Thus, we need to learn and store these parameters, which we can do using `scikit-learn` and `Feature-engine`'s `transformers`\n",
    "\n",
    "> <font color=cyan>If the percentage of missing values is high, frequent category imputation may distort the original distribution of categories.</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's do the importations\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "\n",
    "# 2 Let's load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "\n",
    "# 3 Frequent categories should be calculated using the train set variables, so let's separate the data\n",
    "# into train and test sets and their respective targets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "\n",
    "# 4 Let's replace missing values with the frequent category category, taht is, the mode,\n",
    "# in four categorical variables\n",
    "for var in ['A4', 'A5', 'A6', 'A7']:\n",
    "    value = X_train[var].mode()[0]\n",
    "    X_train[var] = X_train[var].fillna(value)\n",
    "    X_test[var] = X_test[var].fillna(value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The pandas' `fillna()` returns a new dataset with imputed values by default. Instead of doing this, we can replace missing data in the original dataframe by executing `X_train[var].fillna(inplace=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 First, let's separate the original dataset into train and test sets and only retain the categorical variables:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['A4', 'A5', 'A6', 'A7']], data['A16'], test_size=.3, random_state=0)\n",
    "# 6 Let's create a frequent category imputer with SimpleImputer() from scikit-learn\n",
    "imputer = SimpleImputer(strategy='most_frequent')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SimpleImputer()` from scikit-learn **will learn the mode for numerical and categorical variables alike**. But **in practice, mode imputation is done for categorical variables only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['u', 'g', 'c', 'v'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7 Let's fit the imputer to the train set so that it learns the most frequent values\n",
    "imputer.fit(X_train)\n",
    "# 8 Let's inspect the most frequent values learned by the imputer\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 Let's replace missing values with frequent categories\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `SimpleImputer()` will return a NumPy array and not a pandas dataframe.\n",
    "Finally, let's impute missing values using Feature-engine. First, we need to load and separate the data into train and test sets, just like we did in step 2 and step 3 in this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['A4', 'A5', 'A6', 'A7']], data['A16'], test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Next, let's create a frequent category imputer with CategoricalImputer\n",
    "mode_imputer = CategoricalImputer(variables=['A4', 'A5', 'A6', 'A7'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CategorcalImputer()` will select all categorical variables in the train set by default; that is, unless we pass a list of variables to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A4': 'Missing', 'A5': 'Missing', 'A6': 'Missing', 'A7': 'Missing'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11 Let's fit the imputation transformer to the train set so that it learns the most\n",
    "# frequent categories\n",
    "mode_imputer.fit(X_train)\n",
    "# 12 Let's inspect the learned frequent categories\n",
    "mode_imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13 Finally, let's replace the missing values with frequent categories:\n",
    "X_train = mode_imputer.transform(X_train)\n",
    "X_test = mode_imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CategoricalImputer()` returns a pandas dataframe with the imputed values.\n",
    "\n",
    "Remember that you can check that the categorical variables do not contain missing values by using `X_train[['A4', 'A5', 'A6', 'A7']].isnull().mean()`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Replacing missing values with an arbitrary number**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbitrary number imputation consists of replacing missing values with an arbitrary value. Some commonly used values include $999$, $9999$, or $-1$ for positive distributions. This method is suitable for numerical variables. A similar method for categorical variables will be discussed in the *Capturing missing values in a bespoke category* recipe.\n",
    "\n",
    "When replacing missing values with an arbitrary number, **we need to be careful not to select a value close to the mean or the median, or any other common value of the distribution**.\n",
    "\n",
    "> Arbitrary number imputation can be used when data is not missing at random, when we are building non-linear models, and when the percentage of missing data is high. This imputation technique distorts the original variable distribution."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A2     76.750\n",
       "A3     26.335\n",
       "A8     20.000\n",
       "A11    67.000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Import pandas and the required functions and classes from scikit-learn and Feature-engine:\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.imputation import ArbitraryNumberImputer\n",
    "# 2 Let's load the dataset:\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Let's separate the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
    "# Normally, we select arbitrary values that are bigger than the maximum value of the distribution.\n",
    "# Let's find the maximum value of four numerical variables:\n",
    "X_train[['A2','A3', 'A8', 'A11']].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Let's replace the missing values with 99 in the numerical variables that we specified in step 4:\n",
    "for var in ['A2','A3', 'A8', 'A11']:\n",
    "    X_train[var].fillna(99, inplace=True)\n",
    "    X_test[var].fillna(99, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll impute missing values with an arbitrary number using scikit-learn instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 First, let's separate the data into train and test sets while keeping only the numerical variables:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['A2', 'A3', 'A8', 'A11']], data['A16'], test_size=0.3, random_state=0)\n",
    "# 7 Let's set up SimpleImputer() so that it replaces any missing values with 99:\n",
    "imputer = SimpleImputer(strategy='constant', fill_value=99)\n",
    "# 8 Let's fit the imputer to the train set:\n",
    "imputer.fit(X_train)\n",
    "# 9 Let's replace the missing values with 99:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o finish, let's impute missing values using Feature-engine. First, we need to load the data and separate it into train and test sets, just like we did in step 2 and step 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[['A2', 'A3', 'A8', 'A11']], data['A16'], test_size=0.3, random_state=0)\n",
    "\n",
    "# 10 Next, let's create an imputation transformer with Feature-engine's ArbitraryNumberImputer() in order to replace any missing values with 99 and specify the variables from which missing data should be imputed:\n",
    "imputer = ArbitraryNumberImputer(arbitrary_number=99, variables=['A2','A3', 'A8', 'A11']) \n",
    "# ArbitraryNumberImputer() will automatically select all numerical variables in the train set; that is, unless we specify which variables to impute in a list.\n",
    "# 11 Let's fit the arbitrary number imputer to the train set:\n",
    "imputer.fit(X_train)\n",
    "# 12 Finally, let's replace the missing values with 99:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Capturing missing values in a bespoke category**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data in categorical variables can be treated as a different category, so it is common to replace missing values with the Missing string. In this recipe, we will learn how to do so using pandas, scikit-learn, and Feature-engine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SimpleImputer(fill_value=&#x27;Missing&#x27;, strategy=&#x27;constant&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(fill_value=&#x27;Missing&#x27;, strategy=&#x27;constant&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SimpleImputer(fill_value='Missing', strategy='constant')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Import pandas and the required functions and classes from scikit-learn and Feature-engine:\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from feature_engine.imputation import CategoricalImputer\n",
    "# 2 Let's load the dataset:\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Let's separate the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
    "# 4 Let's replace missing values in four categorical variables by using the Missing string:\n",
    "for var in ['A4', 'A5', 'A6', 'A7']:\n",
    "    X_train[var].fillna('Missing', inplace=True)\n",
    "    X_test[var].fillna('Missing', inplace=True)\n",
    "# Alternatively, we can replace missing values with the Missing string using scikit-learn as follows.\n",
    "\n",
    "# 5 First, let's separate the data into train and test sets while keeping only categorical variables:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[['A4', 'A5', 'A6', 'A7']], data['A16'], test_size=0.3, random_state=0)\n",
    "# 6 Let's set up SimpleImputer() so that it replaces missing data with the Missing string and fit it to the train set:\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `impleImputer()` from scikit-learn **will replace missing values with Missing in both numerical and categorical variables**. Be careful of this behavior or you will end up accidentally casting your numerical variables as objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Let's replace the missing values:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Remember that SimpleImputer() returns a NumPy array, which you can transform into a dataframe using `pd.DataFrame(X_train, columns = ['A4', 'A5', 'A6', 'A7'])`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finish, let's impute missing values using Feature-engine. First, we need to separate the dataset, just like we did in step 3 of this recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
    "# 8 Next, let's set up the CategoricalVariableImputer() from Feature-engine, which replaces missing values with the Missing string, specifying the categorical variables to impute, and then fit the transformer to the train set:\n",
    "imputer = CategoricalImputer(variables=['A4', 'A5', 'A6', 'A7'])\n",
    "imputer.fit(X_train)\n",
    "# If we don't pass a list with categorical variables, FrequentCategoryImputer() will select all categorical variables in the train set.\n",
    "# 9 Finally, let's replace the missing values:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Replacing missing values with a value at the end of the distribution**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replacing missing values with a value at the end of the variable distribution is equivalent to replacing them with an arbitrary value, but instead of identifying the arbitrary values manually, these values are automatically selected as those at the very end of the variable distribution. The values that are used to replace missing information are estimated using the mean plus or minus three times the standard deviation if the variable is normally distributed, or the inter-quartile range $(IQR)$ proximity rule otherwise. According to the IQR proximity rule, missing values will be replaced with the $\\text{ 75th quantile} + (IQR \\times 1.5)$ at the right tail or by the $\\text{25th quantile} - (IQR \\times 1.5)$ at the left tail. The IQR is given by the $\\text{75th quantile} - \\text{the 25th quantile}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Some users will also identify the minimum or maximum values of the variable and replace missing data as a factor of these values, for example, three times the maximum value.\n",
    "\n",
    "The value that's used to replace missing information should be learned from the train set and stored to impute train, test, and future data. Feature-engine offers this functionality. In this recipe, we will implement end-of-tail imputation using pandas and Feature-engine.\n",
    "\n",
    "> End-of-tail imputation may distort the distribution of the original variables, so it may not be suitable for linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's import the libs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.imputation import EndTailImputer\n",
    "# 2 Let's load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Let's separate the data intro train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "# 4 Let's loop over 5 numerical variables, calculate IQR, determine the value of the 75th quantile\n",
    "# plus 1.5 times the IQR, and replace the missing observations in the train and test sets with that\n",
    "# value\n",
    "for var in ['A2', 'A3', 'A8', 'A11', 'A15']:\n",
    "    IQR = X_train[var].quantile(0.75) - X_train[var].quantile(0.25)\n",
    "    value = X_train[var].quantile(0.75) + 1.5 * IQR\n",
    "\n",
    "    X_train[var] = X_train[var].fillna(value)\n",
    "    X_test[var] = X_test[var].fillna(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>202.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>43.0</td>\n",
       "      <td>560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>280.0</td>\n",
       "      <td>824</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>100.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  A1     A2     A3 A4 A5 A6 A7    A8   A9  A10  A11 A12 A13    A14  A15  A16\n",
       "0  b  30.83  0.000  u  g  w  v  1.25    t    t    1   f   g  202.0    0    1\n",
       "1  a  58.67  4.460  u  g  q  h  3.04    t    t    6   f   g   43.0  560    1\n",
       "2  a  24.50    NaN  u  g  q  h   NaN  NaN  NaN    0   f   g  280.0  824    1\n",
       "3  b  27.83  1.540  u  g  w  v  3.75    t    t    5   t   g  100.0    3    1\n",
       "4  b  20.17  5.625  u  g  w  v  1.71    t    f    0   f   s  120.0    0    1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we want to use the Gaussian approximation instead of the IQR proximity rule, we can calculate the value to replace missing data using `value = X_train[var].mean() + 3*X_train[var].std()`. Some users also calculate the value as `X_train[var].max()*3`.\n",
    "\n",
    "Note how we calculated the value to impute the missing data using the variables in the train set and then used this to impute train and test sets.\n",
    "\n",
    "We can also place replace missing data with values at the left tail of the distribution using `value = X_train[var].quantile(0.25) - 1.5 * IQR` or `value = X_train[var].mean() - 3*X_train[var].std()`.\n",
    "\n",
    "To finish, let's impute missing values using Feature-engine. First, we need to load and separate the data into train and test sets, just like in step 2 and step 3 of this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Next, let's set up EndTailImputer() so that we can estimate a value at the right tail using the IQR proximity rule and specify the variables we wish to impute:\n",
    "imputer = EndTailImputer(imputation_method='gaussian', tail='right', variables=['A2', 'A3', 'A8', 'A11', 'A15'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use mean and standard deviation to calculate the replacement values, we need to set `imputation_method='iqr'`. We can use `'left'` or `'right'` in the `tail` argument to specify the side of the distribution where we'll place the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A2': 71.35031102042569,\n",
       " 'A3': 25.854124379795604,\n",
       " 'A8': 13.675535600625327,\n",
       " 'A11': 18.320547522636208,\n",
       " 'A15': 12740.850618383236}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6 Let's fit the EndTailImputer() to the train set so that it learns the parameters:\n",
    "imputer.fit(X_train)\n",
    "# 7 Let's inspect the learned values:\n",
    "imputer.imputer_dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 Finally let's replace the missing values\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A2     0\n",
       "A3     0\n",
       "A8     0\n",
       "A11    0\n",
       "A15    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train[['A2', 'A3', 'A8', 'A11', 'A15']].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, **we replaced the missing values in numerical variables with a value at the end of the distribution using pandas and Feature-engine**. These values were **calculated using the IQR proximity rule or the mean and standard deviation**. First, we loaded the data and divided it into train and test sets using `train_test_split()`, as described in the Performing mean or median imputation recipe."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation random sample imputation**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random sampling imputation consists of extracting random observations from the pool of available values in the variable**. Random sampling imputation preserves the original distribution, which differs from the other imputation techniques we've discussed in this chapter and is suitable for numerical and categorical variables alike."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Let's imoport the libs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from feature_engine.imputation import RandomSampleImputer\n",
    "# 2 Let's load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 The random values that will be used to replace missing data\n",
    "# should be extracted from the train set, so let's separate the\n",
    "# data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "# 4 Let's calculate the number of missing values in the A2 variable\n",
    "number_na = X_train['A2'].isnull().sum()\n",
    "number_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Let's extract 11 values at random from A2 for the imputation\n",
    "random_sample_train = X_train['A2'].dropna().sample(number_na, random_state=0)\n",
    "# 6 We can only use one pandas Series to replace values in another pandas\n",
    "# Series if their indexes are identical, so let's re-index the extracted\n",
    "# random values so that they match the index of the missing values in the\n",
    "# original dataframe:\n",
    "random_sample_train.index = X_train[X_train['A2'].isnull()].index\n",
    "# 7 Now let's replace the missing values in the original dataset with randomly\n",
    "# extracted values\n",
    "X_train.loc[X_train['A2'].isnull(), 'A2'] = random_sample_train\n",
    "# 8 Now, let's combine step 4 to step 7 in a loop to replace the missing data\n",
    "# in the variables in various train and test sets:\n",
    "for var in ['A1', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8']:\n",
    "    # extract a random sample\n",
    "    random_sample_train = X_train[var].dropna().sample(X_train[var].isnull().sum(), random_state=0)\n",
    "    random_sample_test = X_train[var].dropna().sample(X_test[var].isnull().sum(), random_state=0)\n",
    "    # re-index the randomly extracted sample\n",
    "    random_sample_train.index = X_train[X_train[var].isnull()].index\n",
    "    random_sample_test.index = X_test[X_test[var].isnull()].index\n",
    "    # replace the NA\n",
    "    X_train.loc[X_train[var].isnull(), var] = random_sample_train\n",
    "    X_test.loc[X_test[var].isnull(), var] = random_sample_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note how we always extract values from the train set, but we calculate the number of missing values and the index using the train or test sets, respectively.\n",
    "\n",
    "To finish, let's impute missing values using Feature-engine. First, we need to separate the data into train and test, just like we did in step 3 of this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomSampleImputer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomSampleImputer</label><div class=\"sk-toggleable__content\"><pre>RandomSampleImputer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomSampleImputer()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 Next, let's set up RandomSamplemputer() and fit it to the train set:\n",
    "imputer = RandomSampleImputer()\n",
    "imputer.fit(X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `RandomSampleImputer()` will replace the values in all variables in the dataset by default.\n",
    "\n",
    "We can specify the variables to impute by passing variable names in a list to the imputer using `imputer = RandomSampleImputer(variables = ['A2', 'A3'])`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Finally, let's replace the missing values:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Adding a missing value indicator variable**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A missing indicator is a binary variable that specifies whether a value was missing for an observation ($1$) or not ($0$)**. **It is common practice to replace missing observations by the mean, median, or mode while flagging those missing observations with a missing indicator, thus covering two angles: if the data was missing at random, this would be contemplated by the mean, median, or mode imputation, and if it wasn't, this would be captured by the missing indicator**. In this recipe, we will learn how to add missing indicators using NumPy, scikit-learn, and Feature-engine"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting ready**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example of the implementation of missing indicators, along with mean imputation, check out the Winning the *KDD Cup Orange Challenge with Ensemble Selection* article, which was the winning solution in the KDD 2009 cup: http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A1_NA</th>\n",
       "      <th>A3_NA</th>\n",
       "      <th>A4_NA</th>\n",
       "      <th>A5_NA</th>\n",
       "      <th>A6_NA</th>\n",
       "      <th>A7_NA</th>\n",
       "      <th>A8_NA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>a</td>\n",
       "      <td>46.08</td>\n",
       "      <td>3.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.375</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>g</td>\n",
       "      <td>396.0</td>\n",
       "      <td>4159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>a</td>\n",
       "      <td>15.92</td>\n",
       "      <td>2.875</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>v</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>g</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>b</td>\n",
       "      <td>36.33</td>\n",
       "      <td>2.125</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>0.085</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>g</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1187</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>b</td>\n",
       "      <td>22.17</td>\n",
       "      <td>0.585</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>...</td>\n",
       "      <td>g</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>b</td>\n",
       "      <td>57.83</td>\n",
       "      <td>7.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>14.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>...</td>\n",
       "      <td>g</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1332</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    A1     A2     A3 A4 A5  A6  A7      A8 A9 A10  ...  A13    A14   A15  \\\n",
       "596  a  46.08  3.000  u  g   c   v   2.375  t   t  ...    g  396.0  4159   \n",
       "303  a  15.92  2.875  u  g   q   v   0.085  f   f  ...    g  120.0     0   \n",
       "204  b  36.33  2.125  y  p   w   v   0.085  t   t  ...    g   50.0  1187   \n",
       "351  b  22.17  0.585  y  p  ff  ff   0.000  f   f  ...    g  100.0     0   \n",
       "118  b  57.83  7.040  u  g   m   v  14.000  t   t  ...    g  360.0  1332   \n",
       "\n",
       "     A1_NA  A3_NA  A4_NA  A5_NA  A6_NA  A7_NA  A8_NA  \n",
       "596      0      0      0      0      0      0      0  \n",
       "303      0      0      0      0      0      0      0  \n",
       "204      0      0      0      0      0      0      0  \n",
       "351      0      0      0      0      0      0      0  \n",
       "118      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Let's import the required libs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import MissingIndicator\n",
    "from feature_engine.imputation import AddMissingIndicator\n",
    "# 2 Let's separate the data into and tes sets\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Let's separate the data into train and test sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
    "# 4 Using NumPy, we'll add a missing indicator to the numerical and categorical variables in a loop:\n",
    "for var in ['A1', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8']:\n",
    "    X_train[var + '_NA'] = np.where(X_train[var].isnull(), 1, 0)\n",
    "    X_test[var + '_NA'] = np.where(X_test[var].isnull(), 1, 0)\n",
    "# 5 Let's inspect the result of the preceding code block:\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add missing indicators using Feature-engine instead. First, we need to load and divide the data, just like we did in step 2 and step 3 of this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Next, let's set up a transformer that will add binary indicators to all the variables in the dataset using AddNaNBinaryImputer() from Feature-engine:\n",
    "imputer = AddMissingIndicator()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can specify the variables which should have missing indicators by passing the variable names in a list: `imputer = AddMissingIndicator(variables = ['A2', 'A3'])`. Alternatively, the imputer will add indicators to all the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 Let's fit AddNaNBinaryImputer() to the train set:\n",
    "imputer.fit(X_train)\n",
    "# 8 Finally, let's add the missing indicators:\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can inspect the result using `X_train.head()`; it should be similar to the output of step 5 in this recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MissingIndicator()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MissingIndicator</label><div class=\"sk-toggleable__content\"><pre>MissingIndicator()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MissingIndicator()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9 Next, we'll set up a MissingIndicator(). Here, we will add indicators only to variables with missing data:\n",
    "indicator = MissingIndicator(features='missing-only')\n",
    "# 10 Let's fit the transformer so that it finds the variables with missing data in the train set:\n",
    "indicator.fit(X_train) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can concatenate the missing indicators that were created by `MissingIndicator()` to the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11 First, let's create a column name for each of the new missing indicators with a list comprehension:\n",
    "indicator_cols = [c + '_NA' for c in X_train.columns[indicator.features_]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `features_` attribute contains the indices of the features for which missing indicators will be added. If we pass these indices to the train set column array, we can get the variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12 Next, let's concatenate the original train set with the missing indicators, which we obtain using the transform method:\n",
    "X_train = pd.concat([\n",
    "    X_train.reset_index(),\n",
    "    pd.DataFrame(indicator.transform(X_train), columns=indicator_cols)],\n",
    "    axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Scikit-learn transformers return NumPy arrays, so to concatenate them into a dataframe, we must cast it as a dataframe using pandas `DataFrame()`.\n",
    "\n",
    "The result of the preceding code block should contain the original variables, plus the indicators."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Performing multivariate imputation by chained equations**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multivariate imputation methods, as opposed to univariate imputation, use the entire set of variables to estimate the missing values. In other words, **the missing values of a variable are modeled based on the other variables in the dataset**. **Multivariate imputation by chained equations (MICE)** is a multiple imputation technique that models each variable with missing values as a function of the remaining variables and uses that estimate for imputation. MICE has the following basic steps:\n",
    "\n",
    "1. A simple univariate imputation is performed for every variable with missing data, for example, **median imputation**.\n",
    "2. One specific variable is selected, say,` var_1`, and the missing values are set back to missing.\n",
    "3. **A model that's used to predict** `var_1` **is built based on the remaining variables** in the dataset.\n",
    "4. The **missing values** of `var_1` are **replaced with the new estimates**.\n",
    "5. Repeat step 2 to step 4 for each of the remaining variables.\n",
    "\n",
    "Once all the variables have been modeled based on the rest, a cycle of imputation is concluded. Step 2 to step 4 are performed multiple times, typically 10 times, and the imputation values after each round are retained. The idea is that, by the end of the cycles, the distribution of the imputation parameters should have converged.\n",
    "\n",
    "> Each variable with missing data can be modeled based on the remaining variable by using multiple approaches, for example, linear regression, Bayes, decision trees, k-nearest neighbors, and random forests."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting ready**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about MICE, take a look at the following links:\n",
    "\n",
    "* *A multivariate technique for multiplying imputing missing values using a sequence of regression models*: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.405.4540&rep=rep1&type=pdf\n",
    "\n",
    "* *Multiple Imputation by Chained Equations: What is it and how does it work?*: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3074241/\n",
    "\n",
    "We will perform MICE imputation using `IterativeImputer()` from scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's import the required python libs\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# 2 Load the dataset with some numerical variables\n",
    "variables = ['A2', 'A3', 'A8', 'A11', 'A14', 'A16']\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv', usecols=variables)\n",
    "\n",
    "\"\"\"\n",
    "The models that will be used to estimate missing values should be built\n",
    "on the train data and used to impute values in the train, test, and\n",
    "future data:\n",
    "\"\"\"\n",
    "# 3 Let's divide the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "# 4 let's create a MICE imputer using Bayes regression as an estimator, specifying\n",
    "# the number of iteration cycles and setting random_state for reproductibility\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), max_iter=10, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `IterativeImputer()` contains other useful arguments. For example, we can specify **the first imputation strategy using the initial_strategy parameter** and **specify how we want to cycle over the variables either randomly**, or **from the one with the fewest missing values to the one with the most**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Let's fit IterativeImputer to the train set so that it trains the estimators\n",
    "# to predict the missing values in each variable\n",
    "imputer.fit(X_train)\n",
    "\n",
    "# 6 Finally, let's fill in missing values in both train and test set\n",
    "X_train = imputer.transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "# Remember that scikit-learn returns NumPy arrays and not dataframes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How it works**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this recipe, we performed MICE using `IterativeImputer()` from scikit-learn. First, we loaded data using pandas `read_csv()` and separated it into train and test sets using scikit-learn's `train_test_split()`. Next, we created a multivariate imputation object using the `IterativeImputer()` from scikit-learn. We specified that we wanted to estimate missing values using Bayes regression and that we wanted to carry out 10 rounds of imputation over the entire dataset. We fitted `IterativeImputer()` to the train set so that each variable was modeled based on the remaining variables in the dataset. Next, we transformed the train and test sets with the `transform()` method in order to replace missing data with their estimates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **There's more**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `IterativeImputer()` from scikit-learn, we can model variables using multiple algorithms, such as Bayes, knn, decision trees and random forests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\miniconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\impute\\_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n",
      "c:\\Users\\HP\\miniconda3\\envs\\sklearn\\lib\\site-packages\\sklearn\\impute\\_iterative.py:713: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 Import the libs\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# 2 Load the data and separate the train and test sets\n",
    "variables = ['A2','A3','A8', 'A11', 'A14', 'A15', 'A16']\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv', usecols=variables)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=0.3, random_state=0)\n",
    "\n",
    "# 3 Buid MICE using different modeling strategies\n",
    "imputer_bayes = IterativeImputer(\n",
    "    estimator=BayesianRidge(),\n",
    "    max_iter=10,\n",
    "    random_state=0)\n",
    "\n",
    "imputer_knn = IterativeImputer(\n",
    "    estimator=KNeighborsRegressor(n_neighbors=5),\n",
    "    max_iter=10,\n",
    "    random_state=0)\n",
    "\n",
    "imputer_nonLin = IterativeImputer(\n",
    "    estimator=DecisionTreeRegressor(\n",
    "        max_features='sqrt', random_state=0),\n",
    "    max_iter=10,\n",
    "    random_state=0)\n",
    "\n",
    "imputer_missForest = IterativeImputer(\n",
    "    estimator=ExtraTreesRegressor(\n",
    "        n_estimators=10, random_state=0),\n",
    "    max_iter=10,\n",
    "    random_state=0)\n",
    "\n",
    "\"\"\"\n",
    "Note how, in the preceding code block,\n",
    "we create four different MICE imputers,\n",
    "each with a different machine learning\n",
    "algorithm which will be used to model every\n",
    "variable based on the remaining variables\n",
    "in the dataset.\n",
    "\"\"\"\n",
    "\n",
    "# 4 Fit the MICE imputers to the train set:\n",
    "imputer_bayes.fit(X_train)\n",
    "imputer_knn.fit(X_train)\n",
    "imputer_nonLin.fit(X_train)\n",
    "imputer_missForest.fit(X_train)\n",
    "# 5 Impute missing values in the train set:\n",
    "X_train_bayes = imputer_bayes.transform(X_train)\n",
    "X_train_knn = imputer_knn.transform(X_train)\n",
    "X_train_nonLin = imputer_nonLin.transform(X_train)\n",
    "X_train_missForest = imputer_missForest.transform(X_train)\n",
    "# 6 Convert the NumPy arrays into dataframes:\n",
    "predictors = [var for var in variables if var !='A16']\n",
    "X_train_bayes = pd.DataFrame(X_train_bayes, columns = predictors)\n",
    "X_train_knn = pd.DataFrame(X_train_knn, columns = predictors)\n",
    "X_train_nonLin = pd.DataFrame(X_train_nonLin, columns = predictors)\n",
    "X_train_missForest = pd.DataFrame(X_train_missForest, columns = predictors)\n",
    "# 7 Plot and compare the results:\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "X_train['A3'].plot(kind='kde', ax=ax, color='blue')\n",
    "X_train_bayes['A3'].plot(kind='kde', ax=ax, color='green')\n",
    "X_train_knn['A3'].plot(kind='kde', ax=ax, color='red')\n",
    "X_train_nonLin['A3'].plot(kind='kde', ax=ax, color='black')\n",
    "X_train_missForest['A3'].plot(kind='kde', ax=ax, color='orange')\n",
    "\n",
    "# add legends\n",
    "lines, labels = ax.get_legend_handles_labels()\n",
    "labels = ['A3 original', 'A3 bayes', 'A3 knn', 'A3 Trees', 'A3 missForest']\n",
    "ax.legend(lines, labels, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding plot, we can see that the different algorithms return slightly different distributions of the original variable."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assembling an imputation pipeline with scikit-lean**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets often **contain a mix of numerical and categorical variables**. In addition, **some variables may contain a few missing data points**, while **others will contain quite a big proportion**. The **mechanisms by which data is missing may also vary among variables**. Thus, **we may wish to perform different imputation procedures for different variables**. In this recipe, we will learn **how to perform different imputation procedures for different feature subsets using scikit-learn**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRemember that scikit-learn transformers return NumPy arrays. The beauty of this procedure\\nis that we can save the preprocessor in one object to perpetuate all the parameters\\nthat are learned by the different transformers\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Let's import the libs\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 2 Load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "\n",
    "# 3 Let's divide the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "# 4 Let's group a subset of columns to which we want to apply different imputation\n",
    "# techniques in lits\n",
    "features_num_arbitrary = ['A3', 'A8']\n",
    "features_num_median = ['A2', 'A14']\n",
    "features_cat_frequent = ['A4', 'A5', 'A6', 'A7']\n",
    "features_cat_missing = ['A1', 'A9', 'A10']\n",
    "# 5 Let's create different imputation transformers using SimpleImputer()\n",
    "# within the scikit-learn pipeline\n",
    "imputer_num_arbitrary = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=99)),\n",
    "])\n",
    "imputer_num_median = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "])\n",
    "imputer_cat_frequent = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "imputer_cat_missing = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "])\n",
    "\n",
    "# 6 Now, let's assemble the pipelines with the imputers within \n",
    "# ColumnTransformer() and assign them to the different feature subsets we created in step 4:\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('imp_num_arbitrary', imputer_num_arbitrary, features_num_arbitrary),\n",
    "    ('imp_num_median', imputer_num_median, features_num_median),\n",
    "    ('imp_cat_frequent', imputer_cat_frequent, features_cat_frequent),\n",
    "    ('imp_cat_missing', imputer_cat_missing, features_cat_missing),\n",
    "], remainder='passthrough')\n",
    "\n",
    "# 7 Next, we need to fit the preprocessor to the train set so that the imputation parameters are learned:\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# 8 Finally let's replace the missing values in the train and test sets:\n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "\n",
    "\"\"\"\n",
    "Remember that scikit-learn transformers return NumPy arrays. The beauty of this procedure\n",
    "is that we can save the preprocessor in one object to perpetuate all the parameters\n",
    "that are learned by the different transformers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.0, 2.375, 46.08, ..., 't', 'g', 4159],\n",
       "       [2.875, 0.085, 15.92, ..., 'f', 'g', 0],\n",
       "       [2.125, 0.085, 36.33, ..., 'f', 'g', 1187],\n",
       "       ...,\n",
       "       [0.665, 1.665, 19.58, ..., 'f', 'g', 5],\n",
       "       [2.29, 2.29, 22.83, ..., 't', 'g', 2384],\n",
       "       [3.29, 3.5, 40.58, ..., 't', 's', 0]], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Assembling an imputation pipeline with Feature-engine**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to do it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Let's import the libs\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from feature_engine.imputation import CategoricalImputer, ArbitraryNumberImputer, MeanMedianImputer\n",
    "# 2 Let's load the dataset\n",
    "data = pd.read_csv('../data/creditApprovalUCI.csv')\n",
    "# 3 Let's divide the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('A16', axis=1), data['A16'], test_size=.3, random_state=0)\n",
    "# 4 Let's create lists with the names of the variables that we want to apply specific\n",
    "# imputation techniques to\n",
    "features_num_arbitrary = ['A3', 'A8']\n",
    "features_num_median = ['A2', 'A14']\n",
    "features_cat_frequent = ['A4', 'A5', 'A6', 'A7']\n",
    "features_cat_missing = ['A1', 'A9', 'A10']\n",
    "# 5 Let's assemble an arbitrary value imputer, a median imputer, a frequent category imputer, and an imputer\n",
    "# to replace any missing values with the Missing string within a scikit-learn pipeline:\n",
    "pipe = Pipeline(steps=[\n",
    "    ('imp_num_arbitrary', ArbitraryNumberImputer(\n",
    "        variables = features_num_arbitrary)),\n",
    "    ('imp_num_median', MeanMedianImputer(\n",
    "        imputation_method = 'median', variables=features_num_median)),\n",
    "    ('imp_cat_frequent', CategoricalImputer(\n",
    "        variables = features_cat_frequent)),\n",
    "    ('imp_cat_missing', CategoricalImputer(\n",
    "        variables=features_cat_missing))\n",
    "])\n",
    "# 6 Let's fit the pipeline to the train set so that each imputer learns and stores the imputation parameters:\n",
    "pipe.fit(X_train)\n",
    "# 7 Finally, let's replace missing values in the train and test sets:\n",
    "X_train = pipe.transform(X_train)\n",
    "X_test = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>a</td>\n",
       "      <td>46.08</td>\n",
       "      <td>3.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.375</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>8</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>396.0</td>\n",
       "      <td>4159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>a</td>\n",
       "      <td>15.92</td>\n",
       "      <td>2.875</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>v</td>\n",
       "      <td>0.085</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>b</td>\n",
       "      <td>36.33</td>\n",
       "      <td>2.125</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>0.085</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>b</td>\n",
       "      <td>22.17</td>\n",
       "      <td>0.585</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>0.000</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>b</td>\n",
       "      <td>57.83</td>\n",
       "      <td>7.040</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>m</td>\n",
       "      <td>v</td>\n",
       "      <td>14.000</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>360.0</td>\n",
       "      <td>1332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    A1     A2     A3 A4 A5  A6  A7      A8 A9 A10  A11 A12 A13    A14   A15\n",
       "596  a  46.08  3.000  u  g   c   v   2.375  t   t    8   t   g  396.0  4159\n",
       "303  a  15.92  2.875  u  g   q   v   0.085  f   f    0   f   g  120.0     0\n",
       "204  b  36.33  2.125  y  p   w   v   0.085  t   t    1   f   g   50.0  1187\n",
       "351  b  22.17  0.585  y  p  ff  ff   0.000  f   f    0   f   g  100.0     0\n",
       "118  b  57.83  7.040  u  g   m   v  14.000  t   t    6   t   g  360.0  1332"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('sklearn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "abba74b63ac07f00554c7676ecb12976ca4959712221a1ddcaf32d27fe78653e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
